{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0686a046-5665-400c-9b1e-903b856430ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import uuid\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import glob\n",
    "import joblib\n",
    "import zipfile\n",
    "import pathlib\n",
    "import warnings\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "from typing import List, Tuple\n",
    "from itertools import combinations\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bfc24b9-e189-4e7f-9fb4-b6f5a210df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c5f6967-6a45-4763-8732-048ea4f94285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 50), (100000,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X, y = make_classification(n_samples=100_000, n_features=50, random_state=42)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573635c-776d-4868-825e-24f25810ef31",
   "metadata": {},
   "source": [
    "# Feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00c8d60-ef2b-478f-967d-d7df8588dc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vec = DictVectorizer()\n",
    "vec.fit_transform(measurements).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24579a-7452-4e72-9547-12ce187f8237",
   "metadata": {},
   "source": [
    "# Feature Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d04e9ec4-4dad-4105-b10e-bc54cfdcbd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 50), (100000,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X_scaled = MinMaxScaler().fit_transform(X)\n",
    "X_scaled = RobustScaler().fit_transform(X)\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "\n",
    "X_scaled.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8715d6-0f4f-44d7-84df-399de6953127",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e061e003-d239-4a3d-915e-62783947310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "binning = KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c80824f-1935-4c7a-a4d7-827206cacccd",
   "metadata": {},
   "source": [
    "# Label Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf1c09c8-33b2-42b1-ba65-2ec758fdb130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# label preprocessing\n",
    "le = LabelEncoder()\n",
    "le = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854ce80d-fefb-4b53-8801-53cb37593a3d",
   "metadata": {},
   "source": [
    "# Feature Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4416459b-c6de-4ac1-a6b6-929f1ce26a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (100000, 50)\n",
      "Transformed shape: (100000, 8)\n",
      "Explained variance ratio: [0.05729842 0.02281557 0.02078212 0.02073949 0.02065715 0.02062331\n",
      " 0.02058258 0.02053005]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=8)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(\"Transformed shape:\", X_pca.shape)\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244c9f59-b7cd-4d39-8911-d0b913ab3e79",
   "metadata": {},
   "source": [
    "# time feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebb4bc69-be70-4328-9911-90a8a6e5dec1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_time[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_time\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39myear\n\u001b[1;32m      2\u001b[0m df_time[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_time[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth\n\u001b[1;32m      3\u001b[0m df_time[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdayofweek\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_time[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdayofweek\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_time' is not defined"
     ]
    }
   ],
   "source": [
    "df_time[\"year\"] = df_time[\"timestamp\"].dt.year\n",
    "df_time[\"month\"] = df_time[\"timestamp\"].dt.month\n",
    "df_time[\"dayofweek\"] = df_time[\"timestamp\"].dt.dayofweek\n",
    "df_time[\"is_weekend\"] = df_time[\"dayofweek\"] >= 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d616c9-cf05-48f3-bf4f-eb3c16a90de6",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3285f96-f8db-4894-a36c-2445984475a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape\n",
    "X_new = SelectKBest(f_classif, k=2).fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9aa00aad-7cf4-4106-8726-c777a93e71a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "807ac46b-4480-40a4-b2f0-9947e7b434a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X, y)\n",
    "clf.feature_importances_  \n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1c6b4-92cf-47c6-92b3-e0d695d7351d",
   "metadata": {},
   "source": [
    "# Regression Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b43645-d8a5-4b57-9c2e-1326e400aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn ≥1.2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Models (linear, neighbors, SVM, trees, boosting)\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor  # fast tree boosting\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Data\n",
    "# ---------------------------\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "num_cols = X.columns  # all numeric here\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Preprocessing blocks\n",
    "# ---------------------------\n",
    "# For models that benefit from scaling\n",
    "pre_scaled = ColumnTransformer([\n",
    "    (\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_cols)\n",
    "])\n",
    "\n",
    "# For tree-based models (no scaling needed)\n",
    "pre_tree = ColumnTransformer([\n",
    "    (\"num\", SimpleImputer(strategy=\"median\"), num_cols)\n",
    "])\n",
    "\n",
    "def pipe(estimator, scaled: bool):\n",
    "    return Pipeline([\n",
    "        (\"pre\", pre_scaled if scaled else pre_tree),\n",
    "        (\"model\", estimator)\n",
    "    ])\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Models to compare\n",
    "# ---------------------------\n",
    "models = {\n",
    "    # scaled\n",
    "    \"Linear\":   pipe(LinearRegression(), True),\n",
    "    \"Ridge\":    pipe(Ridge(alpha=10.0, random_state=42), True),\n",
    "    \"Lasso\":    pipe(Lasso(alpha=0.001, random_state=42, max_iter=10000), True),\n",
    "    \"Elastic\":  pipe(ElasticNet(alpha=0.001, l1_ratio=0.5, random_state=42, max_iter=10000), True),\n",
    "    \"KNN\":      pipe(KNeighborsRegressor(n_neighbors=10), True),\n",
    "    \"SVR\":      pipe(SVR(C=10.0, epsilon=0.1, kernel=\"rbf\"), True),\n",
    "\n",
    "    # trees/boosting (unscaled)\n",
    "    \"RF\":       pipe(RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1), False),\n",
    "    \"ET\":       pipe(ExtraTreesRegressor(n_estimators=400, random_state=42, n_jobs=-1), False),\n",
    "    \"GBRT\":     pipe(GradientBoostingRegressor(random_state=42), False),\n",
    "    \"HGB\":      pipe(HistGradientBoostingRegressor(random_state=42), False),\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# 4) CV evaluation\n",
    "# ---------------------------\n",
    "scoring = {\n",
    "    \"rmse\": make_scorer(mean_squared_error, squared=False),\n",
    "    \"mae\":  make_scorer(mean_absolute_error),\n",
    "    \"r2\":   make_scorer(r2_score),\n",
    "}\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "summary = []\n",
    "for name, estimator in models.items():\n",
    "    cv_res = cross_validate(estimator, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    summary.append({\n",
    "        \"model\": name,\n",
    "        \"rmse_mean\": np.mean(cv_res[\"test_rmse\"]),\n",
    "        \"rmse_std\":  np.std(cv_res[\"test_rmse\"]),\n",
    "        \"mae_mean\":  np.mean(cv_res[\"test_mae\"]),\n",
    "        \"r2_mean\":   np.mean(cv_res[\"test_r2\"]),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary).sort_values(\"rmse_mean\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Fit best model on full training set and test\n",
    "# ---------------------------\n",
    "best_name = summary_df.iloc[0][\"model\"]\n",
    "best_model = models[best_name].fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nBest CV model: {best_name}\")\n",
    "print(f\"Test RMSE: {rmse:.4f} | MAE: {mae:.4f} | R2: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335eb7f-34b1-4ea3-a8a3-bd4e22c4a858",
   "metadata": {},
   "source": [
    "# Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6b8b2e2-bb3d-48f9-8cf2-7c343c3be720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " model  acc_mean  f1_mean  auc_mean  logloss_mean\n",
      "   SVC  0.967033 0.973782  0.996078      0.080202\n",
      "LogReg  0.978022 0.982544  0.995872      0.072316\n",
      "    ET  0.973626 0.978883  0.993602      0.112830\n",
      "   HGB  0.969231 0.975528  0.993292      0.130091\n",
      "   KNN  0.962637 0.970849  0.992157      0.175421\n",
      "  GBDT  0.951648 0.961363  0.991950      0.129491\n",
      "    RF  0.960440 0.968348  0.990557      0.122395\n",
      "\n",
      "Best CV model: SVC\n",
      "Test Accuracy: 0.9824561403508771\n",
      "Test F1: 0.9861111111111112\n",
      "Test ROC AUC: 0.9966931216931216\n",
      "Test LogLoss: 0.07921591211872678\n"
     ]
    }
   ],
   "source": [
    "# sklearn ≥1.2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, accuracy_score, f1_score, roc_auc_score, log_loss\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Data (binary classification)\n",
    "# ---------------------------\n",
    "X_np, y = load_breast_cancer(return_X_y=True, as_frame=False)\n",
    "X = pd.DataFrame(X_np)  # all numeric\n",
    "num_cols = X.columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Preprocessing\n",
    "# ---------------------------\n",
    "pre_scaled = ColumnTransformer([\n",
    "    (\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_cols)\n",
    "])\n",
    "\n",
    "pre_tree = ColumnTransformer([\n",
    "    (\"num\", SimpleImputer(strategy=\"median\"), num_cols)\n",
    "])\n",
    "\n",
    "def pipe(estimator, scaled: bool):\n",
    "    return Pipeline([\n",
    "        (\"pre\", pre_scaled if scaled else pre_tree),\n",
    "        (\"model\", estimator)\n",
    "    ])\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Models to compare\n",
    "# ---------------------------\n",
    "models = {\n",
    "    # scaled (need standardization)\n",
    "    \"LogReg\": pipe(LogisticRegression(max_iter=500, n_jobs=-1, random_state=42), True),\n",
    "    \"KNN\":    pipe(KNeighborsClassifier(n_neighbors=11), True),\n",
    "    \"SVC\":    pipe(SVC(C=2.0, kernel=\"rbf\", probability=True, random_state=42), True),\n",
    "\n",
    "    # trees/boosting (no scaling required)\n",
    "    \"RF\":     pipe(RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1), False),\n",
    "    \"ET\":     pipe(ExtraTreesClassifier(n_estimators=400, random_state=42, n_jobs=-1), False),\n",
    "    \"GBDT\":   pipe(GradientBoostingClassifier(random_state=42), False),\n",
    "    \"HGB\":    pipe(HistGradientBoostingClassifier(random_state=42), False),\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# 4) CV evaluation\n",
    "# ---------------------------\n",
    "scoring = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"f1\":       make_scorer(f1_score),  # binary by default; use average=\"macro\" for multiclass\n",
    "    \"roc_auc\":  \"roc_auc\",\n",
    "    \"logloss\":  \"neg_log_loss\",\n",
    "}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "summary = []\n",
    "for name, estimator in models.items():\n",
    "    cv_res = cross_validate(estimator, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    summary.append({\n",
    "        \"model\": name,\n",
    "        \"acc_mean\":  np.mean(cv_res[\"test_accuracy\"]),\n",
    "        \"f1_mean\":   np.mean(cv_res[\"test_f1\"]),\n",
    "        \"auc_mean\":  np.mean(cv_res[\"test_roc_auc\"]),\n",
    "        \"logloss_mean\": -np.mean(cv_res[\"test_logloss\"]),  # negate back\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary).sort_values(\"auc_mean\", ascending=False)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Fit best model on full train and test\n",
    "# ---------------------------\n",
    "best_name = summary_df.iloc[0][\"model\"]\n",
    "best_model = models[best_name].fit(X_train, y_train)\n",
    "\n",
    "# Use probabilities for AUC/logloss when available\n",
    "try:\n",
    "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "except Exception:\n",
    "    # fallback for models without predict_proba (shouldn’t happen here)\n",
    "    y_prob = best_model.decision_function(X_test)\n",
    "    # squish to (0,1) if needed\n",
    "    from scipy.special import expit\n",
    "    y_prob = expit(y_prob)\n",
    "\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f\"\\nBest CV model: {best_name}\")\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Test F1:\", f1_score(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "print(\"Test LogLoss:\", log_loss(y_test, y_prob))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f96545-8f54-43fe-9e97-0d022b101065",
   "metadata": {},
   "source": [
    "# Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b676d-48d9-40cb-9c37-6281eda3fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    log_loss, brier_score_loss, confusion_matrix, classification_report,\n",
    "    top_k_accuracy_score\n",
    ")\n",
    "\n",
    "# y_true: 1D ints; y_pred: predicted labels; y_prob: prob for positive class\n",
    "acc  = accuracy_score(y_true, y_pred)\n",
    "f1   = f1_score(y_true, y_pred, average=\"binary\")      # \"macro\"/\"micro\" for multiclass\n",
    "auc  = roc_auc_score(y_true, y_prob)                   # binary (use OvR for multiclass probs)\n",
    "ap   = average_precision_score(y_true, y_prob)         # PR AUC (handles imbalance well)\n",
    "ll   = log_loss(y_true, y_proba_all_classes)           # requires prob for all classes\n",
    "bs   = brier_score_loss(y_true, y_prob)                # calibration (lower is better)\n",
    "cm   = confusion_matrix(y_true, y_pred)\n",
    "rep  = classification_report(y_true, y_pred)\n",
    "topk = top_k_accuracy_score(y_true, y_proba_all_classes, k=3)  # multiclass@k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e0b1d-85e6-4b7b-ba29-db7d5a52812e",
   "metadata": {},
   "source": [
    "# Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37089656-de71-4a42-95bd-12e0a2ad0805",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     r2_score, mean_squared_error, mean_absolute_error,\n\u001b[1;32m      3\u001b[0m     median_absolute_error, mean_absolute_percentage_error,\n\u001b[1;32m      4\u001b[0m     explained_variance_score, mean_pinball_loss\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m r2   \u001b[38;5;241m=\u001b[39m r2_score(\u001b[43my_true\u001b[49m, y_pred)\n\u001b[1;32m      8\u001b[0m rmse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_true, y_pred, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m mae  \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_true, y_pred)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    r2_score, mean_squared_error, mean_absolute_error,\n",
    "    median_absolute_error, mean_absolute_percentage_error,\n",
    "    explained_variance_score, mean_pinball_loss\n",
    ")\n",
    "\n",
    "r2   = r2_score(y_true, y_pred)\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "medae= median_absolute_error(y_true, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "evs  = explained_variance_score(y_true, y_pred)\n",
    "q90  = mean_pinball_loss(y_true, y_pred_q90, alpha=0.90)  # quantile/Pinball loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d348239-4b46-4245-8ce2-5b37034ac118",
   "metadata": {},
   "source": [
    "# Clustering Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39beb56-442b-451e-b8e4-35b8f86fc94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score\n",
    "\n",
    "# Unsupervised quality (requires features X and cluster labels)\n",
    "sil  = silhouette_score(X, labels)              # higher is better\n",
    "ch   = calinski_harabasz_score(X, labels)       # higher is better\n",
    "db   = davies_bouldin_score(X, labels)          # lower is better\n",
    "\n",
    "# If you *do* have true labels for evaluation\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "ari = adjusted_rand_score(y_true, labels)\n",
    "nmi = normalized_mutual_info_score(y_true, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "551ed1d1-a12d-4b72-ac72-bd0fde6637e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'select_dtypes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Split feature types\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m num_cols \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_dtypes\u001b[49m(include\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnumber)\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m     12\u001b[0m cat_cols \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mdifference(num_cols)\n\u001b[1;32m     14\u001b[0m pre \u001b[38;5;241m=\u001b[39m ColumnTransformer([\n\u001b[1;32m     15\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m\"\u001b[39m, Pipeline([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m\"\u001b[39m, StandardScaler())]), num_cols),\n\u001b[1;32m     16\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m, OneHotEncoder(handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)),  \u001b[38;5;66;03m# safe for categoricals\u001b[39;00m\n\u001b[1;32m     17\u001b[0m ])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'select_dtypes'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Split feature types\n",
    "num_cols = X.select_dtypes(include=np.number).columns\n",
    "cat_cols = X.columns.difference(num_cols)\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"scaler\", StandardScaler())]), num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\")),  # safe for categoricals\n",
    "])\n",
    "\n",
    "clf = LGBMClassifier(\n",
    "    n_estimators=2000,          # big enough; rely on early stopping in manual CV or reduce if slow\n",
    "    learning_rate=0.05,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"clf\", clf),\n",
    "])\n",
    "\n",
    "param_dist = {\n",
    "    \"clf__num_leaves\":        np.arange(31, 256),\n",
    "    \"clf__min_child_samples\": np.arange(5, 200),\n",
    "    \"clf__subsample\":         np.linspace(0.5, 1.0, 11),\n",
    "    \"clf__colsample_bytree\":  np.linspace(0.5, 1.0, 11),\n",
    "    \"clf__reg_alpha\":         10.0 ** np.linspace(-3, 1, 9),   # L1\n",
    "    \"clf__reg_lambda\":        10.0 ** np.linspace(-3, 1, 9),   # L2\n",
    "    \"clf__learning_rate\":     10.0 ** np.linspace(-2.0, -0.7, 10),\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "search = RandomizedSearchCV(\n",
    "    pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=60,\n",
    "    scoring=make_scorer(roc_auc_score, needs_proba=True),\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    refit=True,\n",
    "    random_state=42,\n",
    ")\n",
    "search.fit(X, y)\n",
    "\n",
    "print(\"Best AUC:\", search.best_score_)\n",
    "print(\"Best params:\", search.best_params_)\n",
    "best_model = search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd6a05ca-eb63-4394-810a-6c46c12b88f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m enable_halving_search_cv  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HalvingRandomSearchCV\n\u001b[1;32m      4\u001b[0m halving \u001b[38;5;241m=\u001b[39m HalvingRandomSearchCV(\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mpipe\u001b[49m,\n\u001b[1;32m      6\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_dist,\n\u001b[1;32m      7\u001b[0m     factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,               \u001b[38;5;66;03m# how aggressively to prune\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[1;32m     10\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     11\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     12\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m halving\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest AUC:\u001b[39m\u001b[38;5;124m\"\u001b[39m, halving\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "halving = HalvingRandomSearchCV(\n",
    "    pipe,\n",
    "    param_distributions=param_dist,\n",
    "    factor=3,               # how aggressively to prune\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "halving.fit(X, y)\n",
    "print(\"Best AUC:\", halving.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec02f50b-42e2-4e03-aea2-f2bee595e8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 01:29:41,018] A new study created in memory with name: no-name-0be7bf26-75de-4739-aa9b-01621f0af954\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ebef2bdf68244e48d575c266b6fd9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-08 01:29:41,036] Trial 0 failed with parameters: {'lr': 0.024520933100169007, 'num_leaves': 204, 'min_child_samples': 30, 'subsample': 0.980222949723482, 'colsample_bytree': 0.7041112579202558, 'reg_alpha': 1.6932713331747067, 'reg_lambda': 2.1667000557877443} because of the following error: AttributeError(\"'numpy.ndarray' object has no attribute 'iloc'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/sahand/home/quera/mapna-ai-2025/venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_249556/2915676693.py\", line 25, in objective\n",
      "    model.fit(X.iloc[tr], y[tr])\n",
      "AttributeError: 'numpy.ndarray' object has no attribute 'iloc'\n",
      "[W 2025-09-08 01:29:41,037] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(scores))\n\u001b[1;32m     30\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_value)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest params:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[0;32m/data/sahand/home/quera/mapna-ai-2025/venv/lib/python3.10/site-packages/optuna/study/study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/sahand/home/quera/mapna-ai-2025/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/data/sahand/home/quera/mapna-ai-2025/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/data/sahand/home/quera/mapna-ai-2025/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:258\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    254\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    257\u001b[0m ):\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[0;32m/data/sahand/home/quera/mapna-ai-2025/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[27], line 25\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tr, va \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# fit on raw X; or wrap the same ColumnTransformer as above\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     model \u001b[38;5;241m=\u001b[39m LGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 25\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[tr], y[tr])\n\u001b[1;32m     26\u001b[0m     p \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X\u001b[38;5;241m.\u001b[39miloc[va])[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     27\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(roc_auc_score(y[va], p))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\":      3000,\n",
    "        \"learning_rate\":     trial.suggest_float(\"lr\", 1e-2, 2e-1, log=True),\n",
    "        \"num_leaves\":        trial.suggest_int(\"num_leaves\", 31, 255),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 200),\n",
    "        \"subsample\":         trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\":  trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\":         trial.suggest_float(\"reg_alpha\", 1e-3, 10, log=True),\n",
    "        \"reg_lambda\":        trial.suggest_float(\"reg_lambda\", 1e-3, 10, log=True),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for tr, va in cv.split(X, y):\n",
    "        # fit on raw X; or wrap the same ColumnTransformer as above\n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(X.iloc[tr], y[tr])\n",
    "        p = model.predict_proba(X.iloc[va])[:, 1]\n",
    "        scores.append(roc_auc_score(y[va], p))\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=60, show_progress_bar=True)\n",
    "print(\"Best score:\", study.best_value)\n",
    "print(\"Best params:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cceff2d-7cd0-4cc0-a860-5cc38013092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(\n",
    "    df: pd.DataFrame,\n",
    "    interactions: bool = True,\n",
    "    nonlinear: bool = True,\n",
    "    nonlinear_funcs: Tuple[str, ...] = (\"square\", \"cube\", \"log\"),\n",
    "    interaction_degree: int = 2,                       # 2 = pairwise (original behavior)\n",
    "    include_ratios: bool = False,                      # add col_i / col_j (i != j)\n",
    "    include_diffs: bool = False,                       # add col_i - col_j\n",
    "    drop_constant: bool = True,                        # drop zero-variance new features\n",
    "    eps: float = 1e-12                                 # numerical stability for log/reciprocal/ratios\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate interaction and nonlinear features from a DataFrame.\n",
    "\n",
    "    Defaults replicate the original behavior:\n",
    "      - pairwise products when interactions=True\n",
    "      - square, cube, (safe) log when nonlinear=True\n",
    "\n",
    "    EXTRA OPTIONS:\n",
    "      - columns: limit to selected columns (default = numeric columns)\n",
    "      - nonlinear_funcs: choose from {\"square\",\"cube\",\"sqrt\",\"log\",\"exp\",\"abs\",\"reciprocal\"}\n",
    "      - interaction_degree: k-way products (k>=2), e.g. 3 => x1*x2*x3\n",
    "      - include_ratios: pairwise ratios x_i / x_j\n",
    "      - include_diffs: pairwise differences x_i - x_j\n",
    "      - return_only_new: return only engineered cols (not the originals)\n",
    "      - drop_constant: remove zero-variance engineered cols\n",
    "    \"\"\"\n",
    "    cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # keep original columns unless return_only_new\n",
    "    df_new = df.copy()\n",
    "\n",
    "    # ------------ Nonlinear transforms ------------\n",
    "    if nonlinear and nonlinear_funcs:\n",
    "        for col in cols:\n",
    "            s = df[col].astype(float)\n",
    "\n",
    "            if \"square\" in nonlinear_funcs:\n",
    "                df_new[f\"{col}^2\"] = s ** 2\n",
    "            if \"cube\" in nonlinear_funcs:\n",
    "                df_new[f\"{col}^3\"] = s ** 3\n",
    "            if \"sqrt\" in nonlinear_funcs:\n",
    "                # sqrt only where non-negative; else NaN\n",
    "                df_new[f\"sqrt_{col}\"] = np.where(s >= 0, np.sqrt(s), np.nan)\n",
    "            if \"abs\" in nonlinear_funcs:\n",
    "                df_new[f\"abs_{col}\"] = np.abs(s)\n",
    "            if \"exp\" in nonlinear_funcs:\n",
    "                df_new[f\"exp_{col}\"] = np.exp(np.clip(s, a_min=None, a_max=50))  # avoid overflow\n",
    "            if \"reciprocal\" in nonlinear_funcs:\n",
    "                df_new[f\"1/{col}\"] = np.where(np.abs(s) > eps, 1.0 / s, np.nan)\n",
    "            if \"log\" in nonlinear_funcs:\n",
    "                # safe log: shift only rows where s <= -1 to keep log1p valid\n",
    "                # (minimal shift per column so s_shifted >= -1 + eps)\n",
    "                min_allowed = (-1 + eps)\n",
    "                shift = 0.0\n",
    "                min_val = float(np.nanmin(s.values))\n",
    "                if min_val <= min_allowed:\n",
    "                    shift = (min_allowed - min_val)\n",
    "                df_new[f\"log_{col}\"] = np.log1p(s + shift)\n",
    "\n",
    "    # ------------ Interaction features ------------\n",
    "    if interactions and interaction_degree >= 2:\n",
    "        # k-way distinct-feature products\n",
    "        for k in range(2, interaction_degree + 1):\n",
    "            for combo in combinations(cols, k):\n",
    "                name = \"*\".join(combo)\n",
    "                prod = np.ones(len(df), dtype=float)\n",
    "                for c in combo:\n",
    "                    prod = prod * df[c].astype(float)\n",
    "                df_new[name] = prod\n",
    "\n",
    "    # ------------ Pairwise ratios & differences ------------\n",
    "    if include_ratios or include_diffs:\n",
    "        for a, b in combinations(cols, 2):\n",
    "            a_s, b_s = df[a].astype(float), df[b].astype(float)\n",
    "            if include_diffs:\n",
    "                df_new[f\"{a}-{b}\"] = a_s - b_s\n",
    "                df_new[f\"{b}-{a}\"] = b_s - a_s\n",
    "            if include_ratios:\n",
    "                df_new[f\"{a}/{b}\"] = np.where(np.abs(b_s) > eps, a_s / b_s, np.nan)\n",
    "                df_new[f\"{b}/{a}\"] = np.where(np.abs(a_s) > eps, b_s / a_s, np.nan)\n",
    "\n",
    "    # ------------ Clean up ------------\n",
    "    # Replace inf with NaN (may appear in ratios/reciprocal/exp)\n",
    "    df_new = df_new.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if drop_constant:\n",
    "        # drop newly created constant columns\n",
    "        constant_cols = [c for c in df_new.columns if df_new[c].nunique(dropna=True) <= 1]\n",
    "        # but keep original columns if return_only_new=False\n",
    "        if return_only_new:\n",
    "            df_new = df_new.drop(columns=constant_cols)\n",
    "        else:\n",
    "            # drop only constants that were NOT in the original frame\n",
    "            extras = set(df_new.columns) - set(df.columns)\n",
    "            to_drop = list(set(constant_cols) & extras)\n",
    "            if to_drop:\n",
    "                df_new = df_new.drop(columns=to_drop)\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed6a5f-b147-4479-919e-ed86e4d69172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "def select_by_target_corr(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series | np.ndarray,\n",
    "    *,\n",
    "    method: str = \"pearson\",          # \"pearson\" or \"spearman\"\n",
    "    top_k: Optional[int] = None,      # keep k strongest features\n",
    "    threshold: Optional[float] = None,# keep |corr| >= threshold\n",
    "    drop_constant: bool = True,\n",
    "    return_scores: bool = False\n",
    ") -> List[str] | Tuple[List[str], pd.Series]:\n",
    "    \"\"\"\n",
    "    Select features by absolute correlation with the target.\n",
    "\n",
    "    - For non-numeric y, factorizes to integers (OK for binary classification).\n",
    "    - Only numeric X columns are considered.\n",
    "\n",
    "    Returns:\n",
    "        - list of selected feature names\n",
    "        - (optional) full absolute correlation Series (sorted desc)\n",
    "    \"\"\"\n",
    "    # 1) numeric features only\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    Xn = X[num_cols].copy()\n",
    "\n",
    "    # 2) drop constant cols\n",
    "    if drop_constant:\n",
    "        const_cols = Xn.columns[Xn.nunique(dropna=True) <= 1]\n",
    "        Xn = Xn.drop(columns=const_cols)\n",
    "\n",
    "    # 3) make y numeric if needed\n",
    "    y = pd.Series(y, index=X.index if isinstance(X, pd.DataFrame) else None)\n",
    "    if not pd.api.types.is_numeric_dtype(y):\n",
    "        y, _ = pd.factorize(y)  # unseen labels at test-time are not handled here\n",
    "        y = pd.Series(y, index=X.index)\n",
    "\n",
    "    # 4) compute correlations (pairwise NaN handling by pandas)\n",
    "    corr = Xn.apply(lambda s: s.corr(y, method=method)).abs().sort_values(ascending=False)\n",
    "\n",
    "    # 5) select by threshold/top_k\n",
    "    selected = corr.index\n",
    "    if threshold is not None:\n",
    "        selected = corr[corr >= float(threshold)].index\n",
    "    if top_k is not None:\n",
    "        selected = selected[:int(top_k)]\n",
    "\n",
    "    selected = list(selected)\n",
    "\n",
    "    return (selected, corr) if return_scores else selected\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
